%-------------------------------------------------------------------------------
% Introduction to the chapter
%-------------------------------------------------------------------------------

\section{Medium Data Toolkit}

\textsc{Big data and its analytics} promises huge benefits in terms of value realisation, cost reduction, insights but it also introduces a numerous pitfalls \cite{gandomi2015beyond}.
With developments in information technology, mobile communications and internet of things, large assemblages of data are readily available leading to immense possibilities in research.
But when we analyse these data at such scale, we also encounter a large amount of added complexity and cost.
Hence it is important to be careful in choosing the methods and tools in dealing with big data where we should look to devise right methods and tools for the right problems.
% Need a graphic showing growth of Big data
Moreover in several disciplines, such as statistics and geography etc., the existing methods and tools are already developed for dealing with large scale data.
These methods along with improvements in hardware has made the processing big data in these disciplines possible without a major changes in workflow.
In the current environment of constant change and growth of sources of data, we cannot afford to lose the opportunity to extract information from them while trying to create a perfect, future proof approach in dealing with them.
We need to move fast with a pragmatic approach where we look at other disciplines and adopt best practices and solutions in them and develop consistent approach for our needs rather than reinventing the wheel.

%-------------------------------------------------------------------------------

In the previous chapters we looked at various methods we devised to collect and process data from Wi-Fi probe requests emitted by phones.
Though we discussed the methods conceptually, we left out the rationale behind choosing the toolkit employed to implement those methods.
In this section we elaborate the thought process and rationale behind these decisions.
We start by discussing the concept of `Big Data' in general and look at previous literature to understand its definition, nature and the challenges they pose.
Then we look at the data-sets we collected through the pilot studies and the `Smart Street Sensor' project and evaluate them in terms of the dimensions of the big data.
We also discuss the challenges faced in dealing with our dataset in detail and try to understand the requirements for devising a toolkit for it.
Finally we put together a toolkit to suit our datasets built from simple small UNIX tools.
% Thoughts on UNIX philosophy and link to it

%-------------------------------------------------------------------------------
% Discussion on what is big data
%-------------------------------------------------------------------------------

\subsection{What is `Big Data'?}

With the proliferation of internet enabled personal devices, we have quickly moved from data sparse environment to a data rich one.
We can even confidently say that we are in an age of data deluge where the amount of data which are collected and stored are increasing exponentially in a very short period of time \cite{kitchin2014big}.
As we saw in the previous chapters collecting large amount of data is quick and easy.
Technological advancements have enabled us to be able to think about utilising such large assemblages of data which would have been impossible even in the recent past.
By providing unprecedented coverage, these large assemblages of data - `Big data', provide us with insights which were not possible before.
They often change our approach and methods employed in entire disciplines.
For example, In computer science, fuelled by the explosion of collected user data, there is a paradigm shift in Artificial Intelligence with the use of data mining, machine learning and deep learning.
It is only time before this paradigm shift pervades social sciences research as well.
In addition to the above advantages, Big data because of their nature also introduce numerous challenges in their collection, storage, analysis and visualisation.
This is not including the enormous additional overhead and complexity introduced when we try to employ big data methods and tools.
If we are not careful, using big data tools and methods for solving 'normal' problem can be counter productive where the advantages realised don't justified the overheads introduced.
Hence it is important to understand the `Big data' nature of the datasets we are dealing with at a granular level and choose the tools and methods without any presumptions.

%-------------------------------------------------------------------------------

The first and foremost challenge we face while discussing big data is its definition.
It is hard to clearly and objectively define `Big data' as it can vary widely based on the discipline and perspective.
What may be `big' in one discipline may not be in another.
The nature of data can also be evaluated in various dimensions and can exhibit different properties in those dimensions. 
`Big data' is generally defined within the context of disciplines, as data which cannot be managed with traditional methods and tools in those disciplines and requires substantial change in the approach of the practitioners.
This definition is too subjective and falls short of giving us more understanding of `Big data'.
One of the most subscribed definition is to define the scale of the data in the dimension of volume - size of the data, velocity - speed of the data and variety  - the complexity of the data \cite{laney20013d}.
This has also been extended to include more dimensions such as, veracity - the reliability or truthfulness of the data, visualisation - the complexity in visual interpretation and presentation of the data, and others such as visibility validity, variability, volatility and value.
There have also been other alternative dimensions proposed such as Cardinality, continuity and complexity \cite{suthaharan2014big}.
However we can consider the core dimensions of data - volume, velocity, variety, veracity and visualisation for evaluating our datasets.
Since not all data is 'Big' in all these dimensions, we need to evaluate the `bigness' of the data in each dimension and consider the associated challenges and solutions.

%-------------------------------------------------------------------------------

The second set of challenges arise while we process the big data, its acquisition, storage, extraction, cleaning, annotation, integration, aggregation, modelling, analysis, visualisation and interpretation.
Challenges in each one of these processing activity arises due to the data being big in one or more dimensions.
The data being big in volume, velocity and variety poses challenges in data acquisition, aggregation, cleaning and analysis \cite{li2016geospatial}. 
These challenges make traditional methods impractical and introduce the need for distributed, crowdsourced collection of data, heavily parallelised computing and application of functional programming concepts.
The unstructured nature of the big data also introduces notable biases which mandate careful consideration, proper calibration and weighting during analysis so that we can understand and remove any uncertainties arising from them.
The data being big in veracity dimension poses significant challenges in its analysis and modelling.
Since simple methods such as linear regression fails in such scenarios, we require complex methods such as support vector machines, neural networks and hidden Markov models which compensate the lack of structure with the volume of data.
With such computationally intensive methods, heavily parallelised high performance computing techniques such as GPU processing become indispensable.
We also face significant challenge in visualising such complex features and methods which not only supports critical decision making but also is indispensable in exploratory analysis.
The volume and velocity of big data makes them hard to visually simplify and digest.
They are especially complex to interpret in the time dimension unless presented in small parts.
Geographic information systems do a good job in visualising complex geographic data but struggle to maintain legibility and meaning when dealing with the temporal dimension.
% Sample for visual complexity, hairball and point clouds
The visualisations of big data need to be highly processed, simplified and interactive to present meaning to the viewer. 
They have to balance between functionality, aesthetics and performance.
Finally, because of the variety, big data creates need for consistent, well engineered standards so that multiple approaches and tools can be employed in tandem.

%-------------------------------------------------------------------------------

Apart from these processing challenges, we also have management challenges associated with big data such as privacy and security, data governance and ownership, data and information sharing, and cost.
Since these big datasets are usually comprehensive, securing them and protecting the privacy of the users becomes a central consideration in any project dealing with them.
In many cases, though the data collected itself may not contain personal information but at these scales, in conjunction with other datasets, it can be used to infer them.
The overall approach, methods, tools should comply with relevant legislation such as GDPR as well as the research ethics of all the stakeholders.
This is especially challenging since these large unstructured datasets exhibit ambiguity of their ownership as well which calls for a clear, transparent and secure way to share them with other stakeholders along with publications of results in a timely, accessible manner.
The associated project management and tracking tools need to be capable of handling these data ownership and sharing concerns as well.

Finally, the biggest challenge we face with big data is the cost in terms of money, resources and time.
Though most of the big data tools are developed openly and distributed freely there can be lot of incidental, non-direct costs associated with collecting, processing and managing data with them.
For example, there are the operational costs collecting data at such scale, network costs moving them, server costs storing and processing them, cost of procuring and supporting specialised tools and the human resource cost in hiring and training people who are capable for dealing with them.
Though there are economies of scale at larger scales, the overall resources required to manage big data effectively can be several folds of what is needed for a traditional dataset. 
This makes it important to look at the data in our hands closely and carefully so that we can make informed decisions on how 'big' it is and choose the methods which are the most suited for such dataset.

%-------------------------------------------------------------------------------
% Discuss the data we have in detail 
%-------------------------------------------------------------------------------

\subsection{How big are the Wi-Fi probe request datasets?}

In this section we take a detailed look at the three sets of Wi-Fi probe requests collected as described in chapter 1 \textbf{Reference} using the 5Vs big data framework.
Our aim is to understand the nature of the data in each dimension and thus evaluate the challenges we face in that specific dimension leading to a bespoke solution..
We look at each set of data in each dimension and try to answer the following questions,

\begin{enumerate}
  \setlength{\itemindent}{2em}
  \itemsep-0.25em
  \item{How can this dimension be measure objectively?}
  \item{How big is the data in terms of the defined measurement?}
  \item{How does it data compare with datasets in other disciplines?}
  \item{How can we describe the size of the data?}
\end{enumerate}

We then combine these isolated evaluations to form a combined description of the datasets. This is then used as the basis for developing a list of requirements for designing the data processing and management toolkit.

\vspace{0.5em}\noindent\textit{Volume}\vspace{0.5em}

\noindent Probe requests data, being dynamic and continuous, cannot be quantified as an absolute static number in terms of volume. 
Hence we use a long term measurement - yearly rate for each sensors instead.
On shorter datasets such as the pilot study, we estimate the yearly volume linearly from the available data.
We standardise this measure as the amount of disk space needed to store the collected data when encoded in text form.
It is important to note that this can be reduced many folds by using compression or binary formats but we chose text since it the de-facto standard for exchanging data.

\begin{table}[h]
  \footnotesize
  \begin{center}
    \begin{tabular}{lccc}
      \toprule
      Study & Maximum & Minimum & Average \\
      \midrule
      Pilot Study & & & \\
      Main Study & & & \\
      Smart Street Sensor & & & \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Comparison of volume of the datasets of Wi-Fi probe requests (GB/year).}
  \label{tab:toolkit:volume}
\end{table}

We can see that on average  of data is generates at a place and project of the size of smart street sensor is estimated to generate around  of data.
This is not a trivial volume of data..
Desktop normal analysis of such volume is next to impossible.
At the same time this is not truly what is meant by Big data as well.
True big data sources in terms of volume can be of  sizes.
They cannot be even stored at a central single location.
Within this interdisciplinary context, we can say that a national level sampled collection of Wi-Fi data is medium at best.
A comparison of data sizes in various fields after being standardised is shown in the figure.
We can see that data sources such as LIDAR, social media, internet sources are several orders of magnitude larger.
Even if we assume the largest possible data possible - One sensor in every retail location in UK, we can assume something along the lines of  This assuming data collected as we did in the main study and xxxx retail locations across UK in average.
To summarise, in the volume dimension Wi-Fi data is 'medium' in size and is expected to remain so in the future.

Figure comparing the typical size of the Wi-Fi dataset to a small dataset and truly big data.

\begin{marginfigure}
\includegraphics{images/cat.jpeg}
\caption{This is a margin figure.}
\label{fig:marginfig}
\end{marginfigure}

\textit{Velocity}

Unlike volume, velocity is defined as the rate of collection data at short term.
It is quantified similar to volume with size on disk per unit time but the time period is smaller.
The Wi-fi probe requests are generated almost continuously at each location across various channels.
The temporal precision is around micro seconds, but for convenience we have collected and aggregated data per minute for the pilot studies and 5 minutes for the Smart street sensor project.
Every location generates around xx GB per minute and in total the project generates xx GB per minute.
This again varies from xx to xx and on average we can say a moderately busy location is expected to generate xx in a minute.
When we compare this to long term data sources such as census, slow data such as sample surveys it looks impressively fast and almost real time, but when we compare to actual real time data, Internet ad click through, Large Hadron Collider or Aviation, it is not as fast.
This comparison is shown in figure.
As we saw with volume, even in velocity the Wi-fi Data can be described as 'medium' data which is aggregated every 5 mins and mostly processed daily batches of xx GB.
We can safely say that there are no real need for low latency real time solution while dealing with this data while at the same time we need to recognise that unlike traditional datasets, we have a steady stream of data which needs to be processed regularly and sustainably.

\begin{table}[h]
  \footnotesize
  \begin{center}
    \begin{tabular}{lccc}
      \toprule
      Study & Maximum & Minimum & Average \\
      \midrule
      Pilot Study & & & \\
      Main Study & & & \\
      Smart Street Sensor & & & \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Comparison of velocity/speed of the datasets of Wi-Fi probe requests (MBps)}
  \label{tab:toolkit:volume}
\end{table}

Figure comparing the typical size of the Wi-Fi dataset to a small dataset and truly big data.

\textit{Variety}

This aspect of the data cannot be quantified in absolute time but has to be subjectively discussed.
Since the origin of the data is in the Wi-Fi standard, the core of the data is very very structured.
Every probe request almost always have a core set of data which is highly structured and remains the same all over the world.
This further propagates as well process the data further.
There are two main sources of variability identified withing the Wi-Fi probe requests

 - Information elements : The standard specifies a good amount of flexible data  which can be encoded in the request detailing the capabilities of the mobile  device.
These information element are implemented as per the manufacturer  discretion.

 - There is a lot of variability in the rate at which the probe requests are  generated.

The former is slowly becoming an unimportant one since most of the manufacturers following apple have started to include almost no information elements in the probe request packet to protect the privacy of users by eliminating the possibility of detailed fingerprinting of devices.
The rate of probe request generation still varies widely for different manufacturers but overall cannot constitute much of the variability.
In terms of standardised footfall counts there is only one Ordinal data point along time intervals with a geographic unit and time.
From the above, we can safely assume that the Wi-Fi data shows no Big-data characteristics in the variety dimension.

\textit{Veracity}

This is the dimension in which the Wi-Fi data exhibits big data characteristics.
This comes from the fact the data is collected in a unstructured way and passively.
The first source of veracity originates from the unreliability of the data collection process.
The data is collected through a network of sensors located in multiple locations which communicate to the central server using 3G mobile data connectivity.
We know from experience that the sensors are unreliable at best and at any given period of time about 10\% of sensors fail to send back data regularly.
More over the sensors are installed and uninstalled regularly as and when the data partners join the project.
This results in a data stream which is often erratic and incomplete with large gaps in them.
In addition to this the sensors need to be rebooted regularly due to issues or updates leading to small gaps as well.
This poses immense challenge when we attempt to aggregate the data.
There is a requirement for cleaning and filling in the gaps of the data.

There is also a lot of variability in the physical location of the sensors and the area of measurement.
The sensors may report higher or lower count due to the way it is installed and due to the context of the location as showed in the data cleaning procedures.
Cite Karlo's work.
This leads to a situation where the accuracy of the data collection varying quite widely across location and times.
There is a question of weather the change in the data is due to actual changes at the location or just the change in the configuration of the device.
For example opening of a mobile shop next door can increase the estimated footfall without any change in footfall at the location.

The final veracity issue is the changing mobile landscape.
Though the wifi probe requests are standardised by IEEE, the mobile manufacturers have started adopting obfuscation techniques to protect the privacy of the users.
This started with randomisation of MAC addresses, removal of information elements and generally getting more sophisticated with new versions of operating system.
There is also the bias of operating system adoption and change in market share between manufacturers.
There is no inherent structure or information on what is changed and how often these changes occur which leads to questions on the continuity of the data over long periods of time.
From the above we can conclude that Wi-Fi data shows Big data characteristics in terms of its veracity and requires tools and methods when aggregating, analysing and modelling it.


\textit{Visualisation and reporting}

Visualisation is closely related to volume, velocity and variety of the data.
The Wi-Fi data due to its non-trivial size and velocity, exhibits similar characteristics and challenges in visualisation.
Since there not much variety in the data-set when we process it into footfall all we are left with is time, location and footfall.
Out of which location and footfall is easy to visualise but the time is the complicated one because of its volume - 2 to 3 years worth of data and granularity - 1/5 minute intervals.
This is really hard to simplify and visualise.
The key here is using approaches that show change efficiently and legibly.
This shows the need for a dynamic, interactive visualisation tools which can deal with continuous change over long periods of time.
There is also need for multiple linked dynamic visualisation platform for separating the scope of the visualisation into manageable units.
The second challenge is the communicating the veracity of the data without distracting from the message.
Finally the 'near real time' aspect of the data needs to taken into consideration while visualising it hence the need for always on, interactive, real time dashboards with geographic capabilities.
Considering the above we can say that in terms of visualisation, Wi-Fi data partially shows big data characteristics.

Figure: Example of too much data in time dimension

Summarising the above discussion, we can say that the Wi-Fi data collected from probe requests is at best a 'medium' size data which shows big data characteristics in terms of its veracity.
Any toolkit devised to be used with it need to be able to deal with its medium volume, velocity and visualisation needs and at the same time need to able to deal with the huge veracity of it.
This leads us to devising a 'medium data toolkit' which can be used in such cases so that not to introduce the cost and complexity introduced by broader big data tools.

Figure spider graph showing the profile of Wi-Fi data.

%-------------------------------------------------------------------------------
% A survey of all tools and methods dealing with big data.
%-------------------------------------------------------------------------------

\subsection{A Survey of Methods and Tools}

In this section, we survey the tools and methods available at various stages of the data processing and management process we discusst he tools with respect to the performance (throughput), flexibility, complexity and cost.
We finally try to devise a toolkit which best suits our data needs.


\textit{Collection}

There are numerous tools available for data collection with network of sensors under the umbrella of internet of things.
The primary consideration in the data collection is the scale of the infrastructure and the cost associated with it.
The smartstreetsensor project uses its own proprietary sensor system which collects data at the location.
The tooling decisions were made with the commercial application in mind and is not entirely relevant to our discussion, but for the research conducted with the data, it is necessary to understand the data collection process and how the toolkit integrates with the rest of the setup.
We start by looking at different approaches in the data collection tools and try to reason the most appropriate solution for the Wi-Fi data.
At the hardware level, the lowest level of tool can be a micro-controller such as audrino with a dedicated hardware module with custom software to collect the exact data needed.
This is time consuming, cumbersome takes a lot of cost to develop, but is very flexible, efficient and cheap to deploy.
On the other end of this spectrum we have end-to-end solutions such as Blix, Walkbase, Ecuclid, Retail next, pygmalios etc where the data set is collected through multiple sources and syndicated into a cleaned source by a thirdparty.
This can be costly and inflexible but quick and easy.
The middle ground on this to deal with a complexity as much as the Wi-Fi data, is to use a single board computer with external modules and use general purpose, tools to build a data collection device.

The toolkit we have adopted consists of RaspberryPi, Linux, tcpdump/tshark and nodejs.
The RaspberryPi and the linux platform it provides is one of the most diverse and general purpose systems available.
On top of which we can build our data collection system with specialised, opensource and free Wi-Fi sniffing tool such as tcpdump, tshark along with a general purpose runtime such as nodejs which provides other functions such as scheduling, personal data obfuscation and data transmission.
This system has a capacity to sniff and transfer large amounts of data and with a 3G module is very versatile in terms of location.

\textit{Storage}

This is one of the most diverse set of area in terms of both methods and tools available.
It has been constantly in development since the beginning of computer systems and is one of the fastest changing landscapes.
The aspects to be considered while choosing the data storage solutions are,

\begin{enumerate}
    \item Speed
    \item Redundancy
    \item Reliability
    \item Cost \& complexity
\end{enumerate}

One of the spectrum is just using file systems for storage.
Though it seems to be primitive, this has a lot of advantages.
Operating systems usually are really good at managing - reading, writing and searching filesystems, They usually have no overhead involved and are efficient.
Hierarchical organisations can be pre-indexed for hierarchical data and finally is very reliable.
But the primary disadvantage is the inability to handle complexity or variety in data.

On the other end of the spectrum is the highly distributed big data systems such as Hadoop HDFS which are built for > petabyte datasets and query them without loss of speed.
There are hybrid file systems which are hadoop compatible as well, Azure blob storage, Amazon S3 cloud storage which can be used a storage/ dump for a large amount of data.

In the middle there are databases, which are built prioritise and balance the database needs.
The two major approaches are the relational databases which are optimised for structured data which are related to each other in tabular format.
They are row heavy databases and are good for high volume, low veracity data which has need for consistency.
SQL databases PostgreSQL, Mysql, SQLserver are examples.
The other approach is the document store databases which are column heavy databases which are optimised for high variety data which doesnot need immediate consistency.
These can be as simple as key-value based databases and as complicated as graph databases.
Mongodb, couchdb, cassandra as examples.
Both these approaches can be scaled/distributed for less redundancy and increased throughput.
The former tend to scale veritcally and the latter scale horizontally.
Some like cassandra are built to be highly distributable.

Finally there are solutions such as Hive and hbase which are database like functionality built on top of distributed file systems combining power of both concepts.
This behaves like a hyper large scale database system and works in conjunction with other big data tools


\begin{table}
    \centering
    \begin{tabular}{|l|l|}
        \hline
        Type & Comment\\
        \hline
        Filesystem & for hierarchical data around 10TB range\\
        Cloud Storage & \textless{}  10TB, can add hdfs stuff, more reliability\\
        Relational DB & 1-5TB, Good for relational Data, Row-wise, Partitioning\\
        Document DB & -10TB, Good for unstructured data, column wise,Clustering\\
        HDFS & \textgreater{} 10 TB, Good for scale and structure\\
        Hive, Hbase & on top of HDFS, bring DB to HDFS\\
        \hline
    \end{tabular}
    \caption{Types and Use of Various Storage Solutions}
    \label{storage_table}
\end{table}


Raw wifi data has temporal hierarchy and is of medium size hence a normal filesystem is sufficiently suitable for its needs.
When the same data is aggregated it loses its scale and is highly strucutred so a relation DBMS is sufficient for it.
In case the project runs longer and more longitudinal analysis had to be done on raw data HDFS needs to be used and if the aggregated data scales to >10TB we can handle it with a timescale db should be suitable.
PostgreSQL is more suitable than other databases because of its better support to geographic data.

\textit{Processing}

The primary considerations while surveying are the volume, velocity and veracity
of the data.
We should be careful to choose the tools which are right for the size.
The perfect tools for a medium size data can be as much as 230x faster than big data tools (ref).
At one end there are Big Data analysis tools such as Hadoop based impementations such as Mapreduce and Spark, Business toosl such as skytree, realtime tools such as storm and samoa, cleaning tools such as Openrefine.
All these tools are optimised for the cluster/grid computing and the processing is heavily parallelised across the clusters.
There is also a lot of overhead asssociated with moving data across clusters and we won't be making up for these overheads until we hit certain size of the data.
As we know Wi-Fi data is not at the scale these tools operate, we can look into how large streams of data are handles in computer science/ systems engineering.
Data processing is done in two stages, the first one is the filtering, cleaning and aggregation of the raw Wi-Fi data and the second step is the analysis and modelling of the aggregated data.
As we saw in (ref) the system tools in combination with parallel processing across CPU cores, can be used and can be actually faster for medium sized data.
The data transfer format is text since it is standardised with utf8 and is easily understood and shared between UNIX tools.
This also helps us in the data sharing and management which is discussed later.
For the first part of the processing - filtering \& cleaning we use the following tools,

\begin{enumerate}

\item \textbf{sed} - streaming text editor. A fully featured text editor which works on  stream of text. The stream is processed usually by each line and is the most  commonly used to search and replace (translating) text streams using regular  expressions.

\item \textbf{Grep} - grep (global regex print) is a special case of sed where we search  the stream for regular expression and print the result. This is usually used  for searching and filtering text streams.

\item \textbf{awk} - This is a turing complete special purpose higher level programming  language which is optimised for sorting, validating and transforming text  streams. It is full featured enough to be able to manage a small text based  database by itself. This is usually used to transform tabular delimited data.

\item \textbf{jq} - This is similar to awk, has a emcascript based scripting language  for transforming text data which is in the JavaScript Object Notation format. These four tools form a core toolkit for tranforming, translating and  filtering data. All these tools are single threaded and need an external tool  to parallelise the processes. For this we can use gnu-parallel.

\item \textbf{parallel} - This is a tool built with perl (citation) which parallelises  the any operation across CPU cores and even across multiple nodes through  secure shell (ssh). This gives us a medium sized cluster which is well suited  dealing with text data stored in a file system.

\end{enumerate}

Bash completes the toolkit to provide a overarching high-level scripting interface to combine all the smaller tools and managing data transfer between them as text streams using the 'pipe' operator.
This along with core bash tools such as sort, uniq can give us a basic data filtering, transformation and aggregation toolkit with a reasonable throughput.
Example, For a normal word count problem, this toolkit can give us a through put of 540MB per minute without parallelisation and with parallelisation this can be improved to 2.5GB per minute.


For complex data cleaning techniques such as filling in the gaps, we can use higher level languages such as R or Python through their scripting environments and linking them to our pipelines using bash.
Security in terms of obfuscation can be done through hashing algorithms implemented by openssl, nodejs and R and for encryption, we can use the gnupg.
The toolkit being open source free software has the added advantage of being secure as well.

\textit{Visualisation}

Tableu, Omniscope.

%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------

\subsection{Conclusions}

To summarise we have done a survey of tools and arrived at the following toolkit

Figure of the data toolkit.


