# Data Toolkit - A Case for 'Medium' Data

## Introduction

Big data promises lots of benefits but iit has a lot of pitfalls as well. The
data is readily available leading to possibilities but when we take up full on
big data analysis, we encounter a lot of added complexity and cost. We need to
be careful in terms of what is required for the data-sets in possession.
Moreover in most disciplines there are existing methods built for large datasets
e.g. geography, statistics etc. There have always been big data in other
disciplines but the improvements in hardware has made the processing of big data
possible so they need no major changes to do big data analysis. We need to
devise the right tools for the right problem/work. In the environment of
constant change and growth, we cannot afford to lose the opportunity of
extracting information while trying to utilise a future proof big data approach.
We need to have a pragmatic approach with a consistent philosophy in our
approach rather than reinventing the wheel. We need to look at other disciplines
and adopt approaches and solutions form them.

In the previous chapters we looked at the various methods devised to collect and
process data from WiFi probe requests emitted by phones. Though we discussed the
methods in detail we left out the rationale behind the toolkit chosen to
implement the methods. In this section we start by discussing the nature of 'Big
Data', we discuss the previous research in the 'Big Data' space to understand
the definition nature and challenges of big data. The we look at our data-sets
collected through the pilot studies and the Smart Street Sensor project closely
and evaluate their nature in terms of the dimensions of the big data. We also
discuss in detail the challenges faced in dealing with such dataset and the
requirements expected from a toolkit to do so. Finally we look across
disciplines, discuss and evaluate approaches and tools available in dealing with
such datasets and their fitness of purpose to do so.

## Big Data Discussion

Big data has become the new hot thing in research. Data has become the enxt oil
and we are in a data deluge moving from a data sparse to data rich environment.
As we saw in the previous chapters there are easy and quick ways to collect
large amount of data. The technological advancements has enabled us to be able
to think about utilising such large assemblages of data. Big data gives us
numerous benefits, it can provide us insights which were not possible before. I
can give comprehensive coverage. It can change the approach and methods in whole
disciplines such as computer science where the shift towards machine learning
and AI which is primarily fuelled by the explosion of data available. It aslo
poses a lot of challenges because of its nature, in analysing and managing it.
In addition to these, tyring to solving 'normal' problem using big data tools
and approaches can introduce enormous overhead and introduce additional
complexity without providing much benefits. It can acutualy counter productiveto
use these methods when they are not necessary. It is absolutely necessary for
any research to understand what is big data and how the subject at hand relates
to big data.

The first and foremost challange in big data is its definition. It is not
clearly defined and the definition can vary widely based on the discipline and
perspective. What is big data in one discipline may not be in another. Data also
has various dimensions in which they can exhibit big data properties in limited
number of dimensions. In general it is defined in contrast to traditional
data. It is generally defined as the data which cannot be dealt with traditional
desktop or server client computing methods and hardware and requires substantial
change in the approach. Laney 2001 defined big data in three dimensions volume,
velocity and variety - the three Vs of big data. These have been extended to
include
